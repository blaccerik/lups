{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "uri = f\"postgresql://erik:erik@localhost:5432/erik_db\"\n",
    "engine = create_engine(uri)\n",
    "session_factory = sessionmaker(bind=engine)\n",
    "Session = scoped_session(session_factory)\n",
    "\n",
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, ARRAY, Float\n",
    "\n",
    "class Sentence(Base):\n",
    "    __tablename__ = 'sentences'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    text = Column(String(255), nullable=False)\n",
    "    vector = Column(ARRAY(Float), nullable=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Drop the existing \"sentences\" table\n",
    "Base.metadata.drop_all(engine)\n",
    "\n",
    "# Recreate the \"sentences\" table\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "def generate_random_text(length):\n",
    "    letters = string.ascii_letters\n",
    "    random_text = ''.join(random.choice(letters) for _ in range(length))\n",
    "    return random_text\n",
    "\n",
    "def generate_random_vector(n):\n",
    "    return [random.random() for _ in range(n)]\n",
    "\n",
    "# Create a session and add the dummy data\n",
    "with Session() as session:\n",
    "    for i in range(5000):\n",
    "        s = Sentence()\n",
    "        s.text = generate_random_text(100)\n",
    "        s.vector = generate_random_vector(100)\n",
    "        session.add(s)\n",
    "    session.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotFittedError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 49\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# with Session() as session:\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m text:\n\u001B[1;32m---> 49\u001B[0m     vectorized_question \u001B[38;5;241m=\u001B[39m \u001B[43mvectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43msent\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(vectorized_question))\n\u001B[0;32m     51\u001B[0m         \u001B[38;5;66;03m# print(sent)\u001B[39;00m\n\u001B[0;32m     52\u001B[0m     \u001B[38;5;66;03m# print(text)\u001B[39;00m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# all_data.extend(text)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\lups\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2155\u001B[0m, in \u001B[0;36mTfidfVectorizer.transform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   2139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\u001B[38;5;28mself\u001B[39m, raw_documents):\n\u001B[0;32m   2140\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transform documents to document-term matrix.\u001B[39;00m\n\u001B[0;32m   2141\u001B[0m \n\u001B[0;32m   2142\u001B[0m \u001B[38;5;124;03m    Uses the vocabulary and document frequencies (df) learned by fit (or\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2153\u001B[0m \u001B[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001B[39;00m\n\u001B[0;32m   2154\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2155\u001B[0m     \u001B[43mcheck_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThe TF-IDF vectorizer is not fitted\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2157\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mtransform(raw_documents)\n\u001B[0;32m   2158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mtransform(X, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\lups\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1390\u001B[0m, in \u001B[0;36mcheck_is_fitted\u001B[1;34m(estimator, attributes, msg, all_or_any)\u001B[0m\n\u001B[0;32m   1385\u001B[0m     fitted \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   1386\u001B[0m         v \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mvars\u001B[39m(estimator) \u001B[38;5;28;01mif\u001B[39;00m v\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m v\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1387\u001B[0m     ]\n\u001B[0;32m   1389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fitted:\n\u001B[1;32m-> 1390\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(msg \u001B[38;5;241m%\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(estimator)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m})\n",
      "\u001B[1;31mNotFittedError\u001B[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# add wikipedia to database\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    THRESHOLD = 60\n",
    "\n",
    "    text = remove_brackets(text)\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    if len(sentences) == 0:\n",
    "        return None\n",
    "\n",
    "    if len(sentences) == 1:\n",
    "        if len(sentences[0]) < THRESHOLD:\n",
    "            return None\n",
    "\n",
    "    # Ensure one space between each word in each sentence\n",
    "    formatted_sentences = [' '.join(sentence.split()) for sentence in sentences]\n",
    "    return formatted_sentences\n",
    "\n",
    "def remove_brackets(string):\n",
    "    # Remove brackets and their contents (including nested brackets)\n",
    "    pattern = r'\\([^()]*\\)|\\[[^\\]]*\\]'\n",
    "    while re.search(pattern, string):\n",
    "        string = re.sub(pattern, '', string)\n",
    "    return string\n",
    "\n",
    "\n",
    "all_data = []\n",
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.readlines()\n",
    "    sentences = []\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    for line in raw_text:\n",
    "        text = clean_text(line)\n",
    "        if text is None:\n",
    "            continue\n",
    "        # with Session() as session:\n",
    "\n",
    "        for sent in text:\n",
    "            vectorized_question = vectorizer.transform([sent])\n",
    "            print(len(vectorized_question))\n",
    "                # print(sent)\n",
    "            # print(text)\n",
    "            # all_data.extend(text)\n",
    "\n",
    "print(all_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def find(target_vector, threshold=0.8, batch_size=32):\n",
    "    close = []\n",
    "    with Session() as session:\n",
    "        offset = 0\n",
    "        batch_count = 0\n",
    "        while True:\n",
    "            sentences = session.query(Sentence).offset(offset).limit(batch_size).all()\n",
    "            if not sentences:\n",
    "                break  # No more sentences, end the loop\n",
    "\n",
    "            for sentence in sentences:\n",
    "                sentence_vector = np.array(sentence.vector)\n",
    "                similarity = 1 - cosine(target_vector, sentence_vector)\n",
    "\n",
    "                if similarity >= threshold:\n",
    "                    close.append(sentence)\n",
    "            offset += batch_size\n",
    "            batch_count += 1\n",
    "        print(f\"Total batches processed: {batch_count}\")\n",
    "    return close"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot 0.1\n",
      "m1 0.37416573867739417\n",
      "m2 0.37416573867739417\n",
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(vector1[i] * vector2[i] for i in range(len(vector1)))\n",
    "    magnitude1 = math.sqrt(sum(vector1[i] ** 2 for i in range(len(vector1))))\n",
    "    magnitude2 = math.sqrt(sum(vector2[i] ** 2 for i in range(len(vector2))))\n",
    "\n",
    "    print(\"dot\", dot_product)\n",
    "    print(\"m1\", magnitude1)\n",
    "    print(\"m2\", magnitude2)\n",
    "\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # To avoid division by zero if any vector has zero magnitude\n",
    "\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "print(cosine_similarity([0.1,0.2,0.3],[0.3,0.2,0.1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
