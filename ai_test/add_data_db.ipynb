{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "import gensim as gensim\n",
    "from sqlalchemy import Column, Integer, String, ARRAY, Float\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "uri = f\"postgresql://erik:erik@localhost:5432/erik_db\"\n",
    "engine = create_engine(uri)\n",
    "session_factory = sessionmaker(bind=engine)\n",
    "Session = scoped_session(session_factory)\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Sentence(Base):\n",
    "    __tablename__ = 'sentences'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    text = Column(String(500), nullable=False)\n",
    "    vector = Column(ARRAY(Float), nullable=False)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Drop the existing \"sentences\" table\n",
    "Base.metadata.drop_all(engine)\n",
    "\n",
    "# Recreate the \"sentences\" table\n",
    "Base.metadata.create_all(engine)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class DataAdder:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.THRESHOLD = 60\n",
    "\n",
    "    def clean_text(self, text):\n",
    "\n",
    "        text = self.remove_brackets(text)\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            return None\n",
    "\n",
    "        if len(sentences) == 1:\n",
    "            if len(sentences[0]) < self.THRESHOLD:\n",
    "                return None\n",
    "\n",
    "        # Ensure one space between each word in each sentence\n",
    "        formatted_sentences = [' '.join(sentence.split()) for sentence in sentences]\n",
    "        return formatted_sentences\n",
    "\n",
    "    def remove_brackets(self, string):\n",
    "        # Remove brackets and their contents (including nested brackets)\n",
    "        pattern = r'\\([^()]*\\)|\\[[^\\]]*\\]'\n",
    "        while re.search(pattern, string):\n",
    "            string = re.sub(pattern, '', string)\n",
    "        return string\n",
    "\n",
    "    def read_file(self, name):\n",
    "        all_data = []\n",
    "        with open(name, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.readlines()\n",
    "            for line in raw_text:\n",
    "                text = self.clean_text(line)\n",
    "                if text is None:\n",
    "                    continue\n",
    "                all_data.extend(text)\n",
    "        return all_data\n",
    "\n",
    "\n",
    "\n",
    "da = DataAdder()\n",
    "all_data = da.read_file(\"data.txt\")\n",
    "print(len(all_data))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "160\n",
      "106\n",
      "128\n",
      "124\n",
      "140\n",
      "174\n",
      "86\n",
      "171\n",
      "192\n",
      "98\n",
      "283\n",
      "111\n",
      "216\n",
      "64\n",
      "96\n",
      "89\n",
      "121\n",
      "111\n",
      "70\n",
      "115\n",
      "184\n",
      "77\n",
      "65\n",
      "77\n",
      "108\n",
      "144\n",
      "239\n",
      "70\n",
      "113\n",
      "142\n",
      "118\n",
      "166\n",
      "262\n",
      "137\n",
      "150\n",
      "162\n",
      "71\n",
      "73\n",
      "229\n",
      "123\n",
      "130\n",
      "184\n",
      "109\n",
      "79\n",
      "254\n",
      "337\n",
      "69\n",
      "98\n",
      "144\n",
      "137\n",
      "138\n",
      "76\n",
      "92\n",
      "122\n",
      "77\n",
      "114\n",
      "290\n",
      "152\n",
      "119\n",
      "157\n",
      "153\n",
      "123\n",
      "133\n",
      "60\n",
      "99\n",
      "73\n",
      "117\n",
      "130\n",
      "81\n",
      "89\n",
      "274\n",
      "144\n",
      "92\n",
      "147\n",
      "89\n",
      "152\n",
      "109\n",
      "73\n",
      "349\n",
      "96\n",
      "137\n",
      "68\n",
      "137\n",
      "236\n",
      "194\n",
      "131\n",
      "174\n",
      "117\n",
      "125\n",
      "89\n",
      "38\n",
      "178\n",
      "150\n",
      "12\n",
      "211\n",
      "79\n",
      "126\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with Session() as session:\n",
    "        for sentence in all_data:\n",
    "            tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            input_ids = torch.tensor([tokens])\n",
    "            outputs = model(input_ids)\n",
    "            vector = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "            s = Sentence()\n",
    "            s.text = sentence\n",
    "            s.vector = vector.tolist()\n",
    "            session.add(s)\n",
    "        session.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5299220765956472 The New Testament was translated into southern Estonian in 1686 .\n",
      "0.5299220765956472 The New Testament was translated into southern Estonian in 1686 .\n",
      "0.5302152957964208 An Estonian grammar book to be used by priests was printed in German in 1637.\n",
      "0.5302152957964208 An Estonian grammar book to be used by priests was printed in German in 1637.\n",
      "0.5446677019350086 The book was a Lutheran manuscript, which never reached the reader and was destroyed immediately after publication.\n",
      "0.5446677019350086 The book was a Lutheran manuscript, which never reached the reader and was destroyed immediately after publication.\n",
      "0.5724760011496554 they created new words out of nothing.\n",
      "0.5724760011496554 they created new words out of nothing.\n",
      "0.6273926247617584 Examples are\n",
      "0.6273926247617584 Examples are\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "class FixedSizeList:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.data = []\n",
    "\n",
    "    def push(self, score, item):\n",
    "        if len(self.data) < self.size:\n",
    "            heapq.heappush(self.data, (score, item))\n",
    "        else:\n",
    "            heapq.heappushpop(self.data, (score, item))\n",
    "\n",
    "    def get_list(self):\n",
    "        return sorted(self.data)\n",
    "\n",
    "question = \"First book?\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokenizer.encode(question, add_special_tokens=True)\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    outputs = model(input_ids)\n",
    "    target_vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# print(target_vector)\n",
    "batch_size = 32\n",
    "fsl = FixedSizeList(10)\n",
    "with Session() as session:\n",
    "    offset = 0\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        sentences = session.query(Sentence).offset(offset).limit(batch_size).all()\n",
    "        if not sentences:\n",
    "            break  # No more sentences, end the loop\n",
    "        for sentence in sentences:\n",
    "            sentence_vector = np.array(sentence.vector)\n",
    "            similarity = 1 - cosine(target_vector, sentence_vector)\n",
    "            fsl.push(similarity, sentence)\n",
    "        offset += batch_size\n",
    "        batch_count += 1\n",
    "\n",
    "for score, sent in fsl.get_list():\n",
    "    print(score, sent.text)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
