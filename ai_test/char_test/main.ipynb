{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import AutoConfig, AutoModelForCausalLM\n",
    "# import torch\n",
    "# import time\n",
    "# import logging\n",
    "#\n",
    "# logger = logging.getLogger(\"test\")\n",
    "#\n",
    "# def load_quant(model,\n",
    "#                checkpoint,\n",
    "#                wbits,\n",
    "#                groupsize=-1,\n",
    "#                faster_kernel=False,\n",
    "#                exclude_layers=None,\n",
    "#                kernel_switch_threshold=128,\n",
    "#                eval=True):\n",
    "#     exclude_layers = exclude_layers or ['lm_head']\n",
    "#\n",
    "#     def noop(*args, **kwargs):\n",
    "#         pass\n",
    "#\n",
    "#     config = AutoConfig.from_pretrained(model, trust_remote_code=False)\n",
    "#     torch.nn.init.kaiming_uniform_ = noop\n",
    "#     torch.nn.init.uniform_ = noop\n",
    "#     torch.nn.init.normal_ = noop\n",
    "#\n",
    "#     torch.set_default_dtype(torch.half)\n",
    "#     transformers.modeling_utils._init_weights = False\n",
    "#     torch.set_default_dtype(torch.half)\n",
    "#     model = AutoModelForCausalLM.from_config(config, trust_remote_code=False)\n",
    "#     torch.set_default_dtype(torch.float)\n",
    "#     if eval:\n",
    "#         model = model.eval()\n",
    "#\n",
    "#     # layers = find_layers(model)\n",
    "#     # for name in exclude_layers:\n",
    "#     #     if name in layers:\n",
    "#     #         del layers[name]\n",
    "#     #\n",
    "#     # if not is_triton:\n",
    "#     #     gptq_args = inspect.getfullargspec(make_quant).args\n",
    "#     #\n",
    "#     #     make_quant_kwargs = {\n",
    "#     #         'module': model,\n",
    "#     #         'names': layers,\n",
    "#     #         'bits': wbits,\n",
    "#     #     }\n",
    "#     #     if 'groupsize' in gptq_args:\n",
    "#     #         make_quant_kwargs['groupsize'] = groupsize\n",
    "#     #     if 'faster' in gptq_args:\n",
    "#     #         make_quant_kwargs['faster'] = faster_kernel\n",
    "#     #     if 'kernel_switch_threshold' in gptq_args:\n",
    "#     #         make_quant_kwargs['kernel_switch_threshold'] = kernel_switch_threshold\n",
    "#     #\n",
    "#     #     make_quant(**make_quant_kwargs)\n",
    "#     # else:\n",
    "#     #     quant.make_quant_linear(model, layers, wbits, groupsize)\n",
    "#     #\n",
    "#     # del layers\n",
    "#     # if checkpoint.endswith('.safetensors'):\n",
    "#     #     from safetensors.torch import load_file as safe_load\n",
    "#     #     model.load_state_dict(safe_load(checkpoint), strict=False)\n",
    "#     # else:\n",
    "#     #     model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "#     #\n",
    "#     # if is_triton:\n",
    "#     #     if shared.args.quant_attn:\n",
    "#     #         quant.make_quant_attn(model)\n",
    "#     #\n",
    "#     #     if eval and shared.args.fused_mlp:\n",
    "#     #         quant.make_fused_mlp(model)\n",
    "#     #\n",
    "#     #     if shared.args.warmup_autotune:\n",
    "#     #         quant.autotune_warmup_linear(model, transpose=not eval)\n",
    "#     #         if eval and shared.args.fused_mlp:\n",
    "#     #             quant.autotune_warmup_fused(model)\n",
    "#     #\n",
    "#     # model.seqlen = 2048\n",
    "#     # return model\n",
    "#\n",
    "# def load_quantized(model_name):\n",
    "#     # if shared.args.model_type is None:\n",
    "#     #     logger.error(\"The model could not be loaded because its type could not be inferred from its name.\")\n",
    "#     #     logger.error(\"Please specify the type manually using the --model_type argument.\")\n",
    "#     #     return None\n",
    "#\n",
    "#     # Select the appropriate load_quant function\n",
    "#     # model_type = shared.args.model_type.lower()\n",
    "#     # if shared.args.pre_layer and model_type == 'llama':\n",
    "#     #     load_quant = llama_inference_offload.load_quant\n",
    "#     # elif model_type in ('llama', 'opt', 'gptj'):\n",
    "#     #     if shared.args.pre_layer:\n",
    "#     #         logger.warning(\"Ignoring --pre_layer because it only works for llama model type.\")\n",
    "#     #\n",
    "#     #     load_quant = _load_quant\n",
    "#     # else:\n",
    "#     #     logger.error(\"Unknown pre-quantized model type specified. Only 'llama', 'opt' and 'gptj' are supported\")\n",
    "#     #     exit()\n",
    "#\n",
    "#     # Find the quantized model weights file (.pt/.safetensors)\n",
    "#     path_to_model = Path(f'{shared.args.model_dir}/{model_name}')\n",
    "#     pt_path = find_quantized_model_file(model_name)\n",
    "#     if not pt_path:\n",
    "#         logger.error(\"Could not find the quantized model in .pt or .safetensors format, exiting...\")\n",
    "#         exit()\n",
    "#     else:\n",
    "#         logger.info(f\"Found the following quantized model: {pt_path}\")\n",
    "#\n",
    "#     # # qwopqwop200's offload\n",
    "#     # if model_type == 'llama' and shared.args.pre_layer:\n",
    "#     #     if len(shared.args.pre_layer) == 1:\n",
    "#     #         pre_layer = shared.args.pre_layer[0]\n",
    "#     #     else:\n",
    "#     #         pre_layer = shared.args.pre_layer\n",
    "#\n",
    "#     model = load_quant(str(path_to_model), str(pt_path))\n",
    "#     # else:\n",
    "#     #     threshold = False if model_type == 'gptj' else 128\n",
    "#     #     model = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\n",
    "#     #\n",
    "#     #     # accelerate offload (doesn't work properly)\n",
    "#     #     if shared.args.gpu_memory or torch.cuda.device_count() > 1:\n",
    "#     #         if shared.args.gpu_memory:\n",
    "#     #             memory_map = list(map(lambda x: x.strip(), shared.args.gpu_memory))\n",
    "#     #             max_cpu_memory = shared.args.cpu_memory.strip() if shared.args.cpu_memory is not None else '99GiB'\n",
    "#     #             max_memory = {}\n",
    "#     #             for i in range(len(memory_map)):\n",
    "#     #                 max_memory[i] = f'{memory_map[i]}GiB' if not re.match('.*ib$', memory_map[i].lower()) else memory_map[i]\n",
    "#     #\n",
    "#     #             max_memory['cpu'] = f'{max_cpu_memory}GiB' if not re.match('.*ib$', max_cpu_memory.lower()) else max_cpu_memory\n",
    "#     #         else:\n",
    "#     #             max_memory = accelerate.utils.get_balanced_memory(model)\n",
    "#     #\n",
    "#     #         device_map = accelerate.infer_auto_device_map(model, max_memory=max_memory, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "#     #         logger.info(\"Using the following device map for the quantized model:\", device_map)\n",
    "#     #         # https://huggingface.co/docs/accelerate/package_reference/big_modeling#accelerate.dispatch_model\n",
    "#     #         model = accelerate.dispatch_model(model, device_map=device_map, offload_buffers=True)\n",
    "#     #\n",
    "#     #     # No offload\n",
    "#     #     elif not shared.args.cpu:\n",
    "#     #         model = model.to(torch.device('cuda:0'))\n",
    "#\n",
    "#     return model\n",
    "#\n",
    "# def load_model(model_name, loader=None):\n",
    "#     logger.info(f\"Loading {model_name}...\")\n",
    "#     t0 = time.time()\n",
    "#\n",
    "#     # loader GPTQ\n",
    "#     model = load_quantized(model_name)\n",
    "#\n",
    "#     tokenizer = \"todo\"\n",
    "#\n",
    "#     # shared.is_seq2seq = False\n",
    "#     # load_func_map = {\n",
    "#     #     'Transformers': huggingface_loader,\n",
    "#     #     'AutoGPTQ': AutoGPTQ_loader,\n",
    "#     #     'GPTQ-for-LLaMa': GPTQ_loader,\n",
    "#     #     'llama.cpp': llamacpp_loader,\n",
    "#     #     'FlexGen': flexgen_loader,\n",
    "#     #     'RWKV': RWKV_loader,\n",
    "#     #     'ExLlama': ExLlama_loader,\n",
    "#     #     'ExLlama_HF': ExLlama_HF_loader\n",
    "#     # }\n",
    "#     #\n",
    "#     # if loader is None:\n",
    "#     #     if shared.args.loader is not None:\n",
    "#     #         loader = shared.args.loader\n",
    "#     #     else:\n",
    "#     #         loader = infer_loader(model_name)\n",
    "#     #         if loader is None:\n",
    "#     #             logger.error('The path to the model does not exist. Exiting.')\n",
    "#     #             return None, None\n",
    "#     #\n",
    "#     # shared.args.loader = loader\n",
    "#     # output = load_func_map[loader](model_name)\n",
    "#     # if type(output) is tuple:\n",
    "#     #     model, tokenizer = output\n",
    "#     # else:\n",
    "#     #     model = output\n",
    "#     #     if model is None:\n",
    "#     #         return None, None\n",
    "#     #     else:\n",
    "#     #         tokenizer = load_tokenizer(model_name, model)\n",
    "#\n",
    "#     # Hijack attention with xformers\n",
    "#     if any((shared.args.xformers, shared.args.sdp_attention)):\n",
    "#         llama_attn_hijack.hijack_llama_attention()\n",
    "#\n",
    "#     logger.info(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\\n\")\n",
    "#     return model, tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 129\u001B[0m\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, tokenizer\n\u001B[0;32m    127\u001B[0m question \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhat color is the sky?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 129\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m7bwizard\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load_tokenizer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m7bwizard\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    131\u001B[0m tokens \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode(question, add_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[1;32mIn[2], line 109\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(model_name, loader)\u001B[0m\n\u001B[0;32m    106\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    108\u001B[0m shared\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mloader \u001B[38;5;241m=\u001B[39m loader\n\u001B[1;32m--> 109\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mload_func_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(output) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[0;32m    111\u001B[0m     model, tokenizer \u001B[38;5;241m=\u001B[39m output\n",
      "Cell \u001B[1;32mIn[2], line 16\u001B[0m, in \u001B[0;36mAutoGPTQ_loader\u001B[1;34m(model_name)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mAutoGPTQ_loader\u001B[39m(model_name):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mAutoGPTQ_loader\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodules\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAutoGPTQ_loader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_quantized\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\lups\\ai_test\\char_test\\modules\\AutoGPTQ_loader.py:83\u001B[0m, in \u001B[0;36mload_quantized\u001B[1;34m(model_name)\u001B[0m\n\u001B[0;32m     81\u001B[0m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     82\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe AutoGPTQ params are: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparams\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 83\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;241m4\u001B[39m\n\u001B[0;32m     84\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoGPTQForCausalLM\u001B[38;5;241m.\u001B[39mfrom_quantized(path_to_model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# These lines fix the multimodal extension when used with AutoGPTQ\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import modules.shared as shared\n",
    "import hashlib\n",
    "\n",
    "print(shared.args.cpu)\n",
    "\n",
    "logger = logging.getLogger(\"test\")\n",
    "\n",
    "def AutoGPTQ_loader(model_name):\n",
    "    import modules.AutoGPTQ_loader\n",
    "\n",
    "    return modules.AutoGPTQ_loader.load_quantized(model_name)\n",
    "\n",
    "def load_tokenizer(model_name, model):\n",
    "    tokenizer = None\n",
    "    path_to_model = Path(f\"{shared.args.model_dir}/{model_name}/\")\n",
    "    if any(s in model_name.lower() for s in ['gpt-4chan', 'gpt4chan']) and Path(f\"{shared.args.model_dir}/gpt-j-6B/\").exists():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Path(f\"{shared.args.model_dir}/gpt-j-6B/\"))\n",
    "    elif path_to_model.exists():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            path_to_model,\n",
    "            trust_remote_code=shared.args.trust_remote_code,\n",
    "            use_fast=False\n",
    "        )\n",
    "\n",
    "    if tokenizer.__class__.__name__ == 'LlamaTokenizer':\n",
    "        pairs = [\n",
    "            ['tokenizer_config.json', '516c6167c884793a738c440e29ccb80c15e1493ffc965affc69a1a8ddef4572a'],\n",
    "            ['special_tokens_map.json', 'ff3b4a612c4e447acb02d40071bddd989fe0da87eb5b7fe0dbadfc4f74de7531']\n",
    "        ]\n",
    "\n",
    "        for pair in pairs:\n",
    "            p = path_to_model / pair[0]\n",
    "            if p.exists():\n",
    "                with open(p, \"rb\") as f:\n",
    "                    bytes = f.read()\n",
    "\n",
    "                file_hash = hashlib.sha256(bytes).hexdigest()\n",
    "                if file_hash != pair[1]:\n",
    "                    logger.warning(f\"{p} is different from the original LlamaTokenizer file. It is either customized or outdated.\")\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_model_settings_from_yamls(model):\n",
    "    settings = shared.model_config\n",
    "    model_settings = {}\n",
    "    for pat in settings:\n",
    "        if re.match(pat.lower(), model.lower()):\n",
    "            for k in settings[pat]:\n",
    "                model_settings[k] = settings[pat][k]\n",
    "\n",
    "    return model_settings\n",
    "\n",
    "def infer_loader(model_name):\n",
    "    path_to_model = Path(f'{shared.args.model_dir}/{model_name}')\n",
    "    model_settings = get_model_settings_from_yamls(model_name)\n",
    "    if not path_to_model.exists():\n",
    "        loader = None\n",
    "    elif Path(f'{shared.args.model_dir}/{model_name}/quantize_config.json').exists() or ('wbits' in model_settings and type(model_settings['wbits']) is int and model_settings['wbits'] > 0):\n",
    "        loader = 'AutoGPTQ'\n",
    "    elif len(list(path_to_model.glob('*ggml*.bin'))) > 0:\n",
    "        loader = 'llama.cpp'\n",
    "    elif re.match('.*ggml.*\\.bin', model_name.lower()):\n",
    "        loader = 'llama.cpp'\n",
    "    elif re.match('.*rwkv.*\\.pth', model_name.lower()):\n",
    "        loader = 'RWKV'\n",
    "    elif shared.args.flexgen:\n",
    "        loader = 'FlexGen'\n",
    "    else:\n",
    "        loader = 'Transformers'\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "def load_model(model_name, loader=None):\n",
    "    logger.info(f\"Loading {model_name}...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    shared.is_seq2seq = False\n",
    "\n",
    "    # using AutoGPTQ\n",
    "\n",
    "    load_func_map = {\n",
    "        # 'Transformers': huggingface_loader,\n",
    "        'AutoGPTQ': AutoGPTQ_loader,\n",
    "        # 'GPTQ-for-LLaMa': GPTQ_loader,\n",
    "        # 'llama.cpp': llamacpp_loader,\n",
    "        # 'FlexGen': flexgen_loader,\n",
    "        # 'RWKV': RWKV_loader,\n",
    "        # 'ExLlama': ExLlama_loader,\n",
    "        # 'ExLlama_HF': ExLlama_HF_loader\n",
    "    }\n",
    "\n",
    "    if loader is None:\n",
    "        if shared.args.loader is not None:\n",
    "            loader = shared.args.loader\n",
    "        else:\n",
    "            loader = infer_loader(model_name)\n",
    "            if loader is None:\n",
    "                logger.error('The path to the model does not exist. Exiting.')\n",
    "                return None, None\n",
    "\n",
    "    shared.args.loader = loader\n",
    "    output = load_func_map[loader](model_name)\n",
    "    if type(output) is tuple:\n",
    "        model, tokenizer = output\n",
    "    else:\n",
    "        model = output\n",
    "        if model is None:\n",
    "            return None, None\n",
    "        else:\n",
    "            tokenizer = load_tokenizer(model_name, model)\n",
    "\n",
    "    # Hijack attention with xformers\n",
    "    if any((shared.args.xformers, shared.args.sdp_attention)):\n",
    "        llama_attn_hijack.hijack_llama_attention()\n",
    "\n",
    "    logger.info(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\\n\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "question = \"what color is the sky?\"\n",
    "\n",
    "model, tokenizer = load_model(\"7bwizard\")\n",
    "tokenizer = load_tokenizer(\"7bwizard\", None)\n",
    "tokens = tokenizer.encode(question, add_special_tokens=True)\n",
    "input_ids = torch.tensor([tokens])\n",
    "print(input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPTQ TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
